# OpenAI Codex Data Scientist Showcase  
*A portfolio demonstrating metrics, evaluation, and analysis for AI-assisted developer tooling*

---

## ðŸŽ¯ Purpose of This Repository

This repo is designed to showcase the **skills required for the Data Scientist, Codex role at OpenAI**:

- Measuring & evaluating AI-assisted code generation  
- Simulating developer telemetry at scale  
- Understanding developer workflows and productivity metrics  
- Running automated code-evaluation pipelines  
- Designing A/B tests and analyzing model differences  
- Communicating insights clearly through notebooks and dashboards  

The goal of this project is to demonstrate **end-to-end ownership** of the DS problems Codex solves every day.

---

## ðŸ“¦ Repository Structure
```bash

openai-codex-ds-showcase/
â”‚
â”œâ”€â”€ developer-telemetry-simulation/
â”‚ â”œâ”€â”€ simulate_telemetry.py
â”‚ â”œâ”€â”€ telemetry_schema.md
â”‚ â””â”€â”€ sample_output.csv
â”‚
â”œâ”€â”€ developer-productivity-analysis/
â”‚ â”œâ”€â”€ productivity_analysis.ipynb
â”‚ â”œâ”€â”€ acceptance_rate_model.py
â”‚ â””â”€â”€ charts/
â”‚
â”œâ”€â”€ code-evaluation-pipeline/
â”‚ â”œâ”€â”€ tasks/
â”‚ â”‚ â”œâ”€â”€ fizzbuzz.py
â”‚ â”‚ â”œâ”€â”€ palindrome.py
â”‚ â”‚ â””â”€â”€ â€¦
â”‚ â”œâ”€â”€ generate_code.py
â”‚ â”œâ”€â”€ run_tests.py
â”‚ â”œâ”€â”€ compute_edit_distance.py
â”‚ â””â”€â”€ evaluation_report.md
â”‚
â”œâ”€â”€ dashboard/
â”‚ â””â”€â”€ app.py
â”‚
â””â”€â”€ README.md <-- (this file)

```


---

## ðŸ§  Skills Demonstrated (Matched to Codex DS Requirements)

### âœ” 1. Understanding Developer Telemetry  
Codex DS analyzes IDE-level signals such as:

- suggestion acceptance rate  
- edit distance between suggestion and final code  
- compile/run failures  
- keystrokes saved  
- latency  
- time-to-completion  
- fallback requests  
- hallucination/failure mode categories  

The repo includes a **synthetic telemetry generator** to model thousands of â€œAI coding sessionsâ€ with configurable behaviors.

---

### âœ” 2. Productivity & Behavioral Analysis  
Notebook includes:

- Acceptance-rate modeling (logistic regression / XGBoost)
- Latency â†’ satisfaction relationships
- Causal inference: *â€œWould this developer have been faster without AI?â€*
- Developer segmentation via clustering
- Fail-case taxonomy analysis

This mirrors how the Codex DS team measures **developer experience and model improvements**.

---

### âœ” 3. Automated Code Evaluation Pipeline  
Codex is evaluated on:

- test pass rates  
- correctness  
- run-time behavior  
- static analysis results  
- refactor/edit distance  
- error types & categories  
- quality deltas between model versions  

This repository includes an automated pipeline that:

1. Sends coding tasks to the OpenAI API  
2. Executes the returned code in a safe sandbox  
3. Runs unit tests  
4. Computes edit distance & quality metrics  
5. Aggregates results into a single evaluation report  

---

### âœ” 4. Experimental Design & A/B Testing  
The analysis includes:

- model A vs. model B acceptance-rate comparisons  
- latency impact on engagement  
- code-eval pass-rate deltas  
- developer-level random effects  
- bootstrapped confidence intervals  

This demonstrates readiness to own **metric design, experiment analysis, and model evaluation** at OpenAI.

---

## ðŸš€ How to Run

### 1. Generate telemetry
```bash
cd developer-telemetry-simulation
python simulate_telemetry.py
```

### 2. Run the productivity analysis notebook

Open:
```bash
developer-productivity-analysis/productivity_analysis.ipynb
```

### 3. Run code evaluation pipeline
```bash
cd code-evaluation-pipeline
python generate_code.py
python run_tests.py
```

### 4. Launch the demo dashboard
```bash
cd dashboard
streamlit run app.py
```

---

### ðŸ§ª Example Metrics Included

Suggestion Acceptance Rate

- Edit Distance From Final Code
- Test Pass Rate
- Code Quality Score
- Compilation Success Ratio
- Time-To-Completion
- Latency Buckets (P50, P90, P99)
- Failure Mode Categorization
- Overall â€œDeveloper Happinessâ€ score

These metrics demonstrate the DS mindset required to improve model performance, developer trust, and product usability.

--- 

## ðŸ§° Technologies Used

- Python
- Jupyter / Colab
- Pandas, NumPy, Scikit-Learn, XGBoost
- Matplotlib / Seaborn / Plotly
- Streamlit Dashboard
- subprocess sandbox for code execution
- OpenAI API (optional)
