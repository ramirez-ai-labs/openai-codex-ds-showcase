# Getting Started: Beginner's Guide to Codex DS Showcase

Welcome! This guide will walk you through the repository step-by-step, explaining what each component does and how they work together.

## ðŸŽ¯ What This Repository Does

This repository simulates the work of a **Data Scientist** at OpenAI working on **Codex** (an AI coding assistant). It demonstrates:

1. **Collecting data** about how developers use AI coding suggestions
2. **Analyzing that data** to understand what works and what doesn't
3. **Comparing different AI models** to see which is better
4. **Evaluating code quality** generated by AI
5. **Building dashboards** to visualize results

Think of it like analyzing how well a new tool helps developers write code faster and better.

---

## ðŸ“‹ Prerequisites

Before you start, make sure you have:
- **Python 3.8+** installed ([Download here](https://www.python.org/downloads/))
- **Git** installed (optional, for cloning)
- A **terminal/command prompt** (PowerShell on Windows, Terminal on Mac/Linux)

---

## ðŸš€ Step 1: Setup Your Environment

### 1.1 Navigate to the Repository

Open your terminal and navigate to the repository folder:

```bash
cd openai-codex-ds-showcase
```

### 1.2 Create a Virtual Environment

A virtual environment keeps your project's dependencies separate from other Python projects.

**Windows:**
```bash
python -m venv .venv
.venv\Scripts\activate
```

**Mac/Linux:**
```bash
python3 -m venv .venv
source .venv/bin/activate
```

You should see `(.venv)` appear at the start of your terminal prompt.

### 1.3 Install Dependencies

Install all the Python packages needed for this project:

```bash
pip install -r requirements.txt
```

This installs:
- `pandas`: For data analysis
- `numpy`: For numerical operations
- `scikit-learn`: For machine learning models
- `streamlit`: For building dashboards
- `plotly`: For interactive charts
- `scipy`: For statistical tests
- `sqlalchemy`: For SQL database connections

**Expected output:** You should see packages being downloaded and installed.

---

## ðŸ“Š Step 2: Understand the Repository Structure

Let's see what's in the repository:

```bash
# Windows
dir

# Mac/Linux
ls
```

You should see these main folders:

```
openai-codex-ds-showcase/
â”œâ”€â”€ app.py                          # Main launcher (start here!)
â”œâ”€â”€ README.md                       # Overview of the project
â”œâ”€â”€ GETTING_STARTED.md              # This file
â”œâ”€â”€ METHODOLOGY.md                  # Deep dive into methods
â”œâ”€â”€ requirements.txt                # Python packages needed
â”‚
â”œâ”€â”€ developer-telemetry-simulation/    # Simulates developer data
â”œâ”€â”€ developer-productivity-analysis/   # Analyzes the data
â”œâ”€â”€ code-evaluation-pipeline/          # Tests AI-generated code
â””â”€â”€ dashboard/                         # Visual dashboard
```

**What each folder does:**
- **telemetry-simulation**: Creates fake data about developers using AI suggestions
- **productivity-analysis**: Analyzes that data to find patterns
- **code-evaluation-pipeline**: Tests if AI-generated code actually works
- **dashboard**: Shows everything in a visual interface

---

## ðŸŽ¬ Step 3: Generate Sample Data (Telemetry Simulation)

### What is Telemetry?

**Telemetry** = data collected about how a system is used. In this case, we're tracking:
- When developers see AI suggestions
- Whether they accept or reject them
- How long suggestions take to appear (latency)
- Whether the code compiles and passes tests

### 3.1 Run the Simulation

```bash
python app.py simulate
```

**What happens:**
1. The script creates a CSV file with ~10,000 rows of fake developer data
2. Each row represents one AI suggestion shown to a developer
3. Data includes: acceptance, latency, language, model version, etc.

**Expected output:**
```
ðŸš€ Running: python developer-telemetry-simulation/simulate_telemetry.py
âœ… Wrote 12345 telemetry rows to telemetry_events.csv
âœ… Done.
```

### 3.2 Explore the Generated Data

Let's look at what was created:

```bash
# Windows
type developer-telemetry-simulation\telemetry_events.csv | more

# Mac/Linux
head developer-telemetry-simulation/telemetry_events.csv
```

You'll see columns like:
- `session_id`: Which coding session
- `developer_id`: Which developer
- `accepted`: Did they accept the suggestion? (True/False)
- `latency_ms`: How long did it take? (milliseconds)
- `language`: What programming language?
- `model_version`: Which AI model version? (v1 or v2)

**Key Insight:** This data simulates what we'd collect from real developers using an AI coding assistant.

---

## ðŸ“ˆ Step 4: Basic Data Analysis

### 4.1 Run Acceptance Rate Analysis

```bash
python app.py analyze
```

**What happens:**
1. Loads the telemetry data
2. Calculates overall acceptance rate
3. Compares acceptance by model version (v1 vs v2)
4. Compares acceptance by user segment (beginner/intermediate/expert)
5. Builds a machine learning model to predict acceptance

**Expected output:**
```
=== Overall Acceptance Rate ===
0.512

=== Acceptance Rate by Model Version ===
model_version
model_v1    0.450
model_v2    0.580

=== Classification Report ===
              precision    recall  f1-score   support
...
```

**What this tells us:**
- Overall, ~51% of suggestions are accepted
- Model v2 has higher acceptance (58%) than v1 (45%)
- The model can predict acceptance with reasonable accuracy

### 4.2 Understanding the Results

**Acceptance Rate**: The percentage of AI suggestions that developers actually use. Higher is better!

**Model Comparison**: We're testing if a new model (v2) is better than the old one (v1). It looks like v2 is winning!

---

## ðŸ’¾ Step 5: SQL Analysis

### What is SQL?

**SQL** = Structured Query Language, used to ask questions of databases. It's like asking: "Show me all suggestions in Python that were accepted."

### 5.1 Run SQL Queries

```bash
python app.py sql
```

**What happens:**
1. Creates a SQLite database from the CSV file
2. Runs 7+ different SQL queries
3. Shows results for each query

**Expected output:**
```
ðŸ“Š Loading telemetry data...
âœ… Created database with 12345 rows

============================================================
Query: Overall Acceptance Rate
============================================================
total_suggestions  accepted_count  acceptance_rate
             12345           6321           0.512
...
```

**Key Queries You'll See:**
1. **Overall metrics**: Total suggestions, acceptance rate
2. **Model comparison**: v1 vs v2 performance
3. **Segmentation**: Acceptance by language, user segment
4. **Session productivity**: How productive are coding sessions?
5. **Error analysis**: What types of errors occur?

**Why SQL matters:** In real companies, data is stored in databases. SQL is how Data Scientists extract insights at scale.

---

## ðŸ”¬ Step 6: A/B Testing (Statistical Comparison)

### What is A/B Testing?

**A/B Testing** = Comparing two versions (A and B) to see which is better, using statistics to ensure the difference is real (not just random chance).

### 6.1 Run A/B Test Analysis

```bash
python app.py abtest
```

**What happens:**
1. Compares model v1 vs v2 on multiple metrics
2. Runs statistical tests (chi-square, t-tests)
3. Calculates p-values (probability the difference is real)
4. Provides recommendations

**Expected output:**
```
======================================================================
A/B TEST RESULTS: Model v1 vs Model v2
======================================================================

ðŸ“Š ACCEPTANCE RATE
----------------------------------------------------------------------
Control (v1):     45.0%
Treatment (v2):  58.0%
Difference:      +13.0%
95% CI:          [+10.1%, +15.9%]
P-value:         0.0001
Significant:     âœ… YES
Interpretation:  Treatment higher than control

======================================================================
RECOMMENDATION:
âœ… Model v2 shows statistically significant improvement. Recommend rollout.
```

**Understanding the Output:**
- **P-value < 0.05**: The difference is statistically significant (not random)
- **95% CI**: We're 95% confident the true difference is in this range
- **Recommendation**: Based on stats, we should use v2

**Key Insight:** This is how Data Scientists make data-driven decisions about which features to ship!

---

## ðŸŽ¯ Step 7: Causal Inference

### What is Causal Inference?

**Causal Inference** = Understanding what *causes* outcomes, not just what's *correlated*.

**Example:** 
- **Correlation**: Higher latency is associated with lower acceptance
- **Causation**: Does latency *cause* lower acceptance, or are both caused by something else (like suggestion quality)?

### 7.1 Run Causal Analysis

```bash
python app.py causal
```

**What happens:**
1. Uses advanced statistical methods (propensity score matching, regression adjustment)
2. Controls for confounding variables
3. Estimates the true causal effect

**Expected output:**
```
======================================================================
CAUSAL INFERENCE ANALYSIS
======================================================================

ðŸ“Š PROPENSITY SCORE MATCHING
----------------------------------------------------------------------
ATE:              +0.130
95% CI:           [+0.101, +0.159]
Matched pairs:    5234
Interpretation:   Treatment increases accepted by 13.0%

ðŸ“ˆ REGRESSION ADJUSTMENT
----------------------------------------------------------------------
ATE:              +0.128
95% CI:           [+0.099, +0.157]
...
```

**What this tells us:**
- After controlling for other factors, v2 still increases acceptance by ~13%
- This is the *true causal effect*, not just correlation
- Multiple methods give similar results (robustness check)

**Key Insight:** This is how Data Scientists prove that a feature actually *causes* improvements, not just correlates with them.

---

## ðŸ’» Step 8: Code Evaluation Pipeline

### What is Code Evaluation?

**Code Evaluation** = Testing if AI-generated code actually works correctly.

### 8.1 Generate Code from AI

```bash
python app.py generate
```

**What happens:**
1. Reads coding tasks from `tasks/tasks.json`
2. For each task, generates code (using a simple placeholder model)
3. Saves generated code to `code_solutions/` folder

**Expected output:**
```
Generating code for task: fizzbuzz
âœ… Wrote code for task fizzbuzz to code_solutions/fizzbuzz.py
Generating code for task: is_palindrome
âœ… Wrote code for task is_palindrome to code_solutions/is_palindrome.py
...
```

**What tasks are evaluated:**
- `fizzbuzz`: Print numbers with Fizz/Buzz rules
- `is_palindrome`: Check if string is palindrome
- `binary_search`: Search in sorted array
- `reverse_linked_list`: Reverse a linked list
- `validate_email`: Validate email format

### 8.2 Test the Generated Code

```bash
python app.py evaluate
```

**What happens:**
1. Imports each generated code file
2. Runs test cases to check correctness
3. Calculates edit distance (how different from reference solution)
4. Classifies failure modes (syntax error, runtime error, logical error)

**Expected output:**
```
ðŸ” Evaluating task: fizzbuzz
  Result: passed, edit_distance=5
ðŸ” Evaluating task: is_palindrome
  Result: passed, edit_distance=8
...

ðŸ“„ Wrote evaluation results to code_eval_results.json
```

**Understanding the Results:**
- **Status**: `passed` = code works correctly, `failed_tests` = code has bugs
- **Edit Distance**: How many character changes needed to match reference. Lower = better.
- **Failure Modes**: Why code failed (syntax, runtime, logic)

**Key Insight:** This is how Data Scientists measure code quality, not just acceptance rate!

---

## ðŸ“Š Step 9: Interactive Dashboard

### What is the Dashboard?

The **Dashboard** = A visual interface to explore all the data and results.

### 9.1 Launch the Dashboard

```bash
python app.py dashboard
```

**What happens:**
1. Starts a Streamlit web server
2. Opens your browser automatically
3. Shows interactive visualizations

**Expected output:**
```
ðŸš€ Running: streamlit run dashboard/app.py

  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://192.168.x.x:8501
```

### 9.2 Explore the Dashboard

The dashboard has 5 pages:

#### **Page 1: Overview**
- High-level metrics at a glance
- Quick comparison charts

#### **Page 2: Code Evaluation**
- Results for each coding task
- Edit distance distributions
- Failure breakdowns

#### **Page 3: Telemetry Analysis**
- Acceptance rates by language, user segment
- Latency distributions
- Interactive filters

#### **Page 4: Model Comparison**
- Side-by-side v1 vs v2 comparison
- Statistical significance indicators
- Recommendations

#### **Page 5: Failure Diagnostics**
- Error type distributions
- Hallucination analysis
- Detailed failure information

**Try This:**
1. Go to "Telemetry Analysis"
2. Use the sidebar filters to select different languages
3. See how acceptance rates change
4. Explore the latency vs acceptance scatter plot

**Key Insight:** This is how Data Scientists make insights accessible to non-technical stakeholders (PMs, Designers, Executives).

---

## ðŸŽ¯ Step 10: Run Everything at Once

Now that you understand each piece, run the full pipeline:

```bash
python app.py all
```

**What happens:**
1. Simulates telemetry data
2. Runs all analyses (basic, SQL, A/B test, causal)
3. Generates and evaluates code
4. Prepares everything for the dashboard

**Expected output:**
```
======================================================================
Running Full Codex DS Showcase Pipeline
======================================================================

ðŸš€ Running: python developer-telemetry-simulation/simulate_telemetry.py
âœ… Done.

ðŸš€ Running: python developer-productivity-analysis/acceptance_rate_model.py
âœ… Done.

... (continues through all steps)

ðŸŽ‰ All tasks completed!
```

Then launch the dashboard to see everything together:
```bash
python app.py dashboard
```

---

## ðŸ“š Step 11: Deep Dive into Understanding

### 11.1 Read the Methodology

```bash
# Open METHODOLOGY.md in your text editor or browser
```

This document explains:
- Why each metric matters
- How experiments are designed
- Statistical methods used
- Best practices

**Key Sections:**
- **Developer Productivity Metrics**: What we measure and why
- **Experiment Design**: How to run proper A/B tests
- **Causal Inference**: Going beyond correlation
- **SQL Analysis**: Query patterns and best practices

### 11.2 Explore the Code

**Start with simple scripts:**
1. `developer-telemetry-simulation/simulate_telemetry.py` - How data is generated
2. `developer-productivity-analysis/acceptance_rate_model.py` - Basic analysis
3. `code-evaluation-pipeline/run_tests.py` - How code is tested

**Then move to advanced:**
1. `developer-productivity-analysis/ab_testing_framework.py` - Statistical tests
2. `developer-productivity-analysis/causal_inference.py` - Causal methods
3. `dashboard/app.py` - Dashboard implementation

### 11.3 Look at the Data

**Explore the CSV:**
```bash
# Open in Excel, Google Sheets, or Python
python -c "import pandas as pd; df = pd.read_csv('developer-telemetry-simulation/telemetry_events.csv'); print(df.head()); print(df.describe())"
```

**Explore the SQL database:**
```bash
# If you have SQLite installed
sqlite3 developer-productivity-analysis/telemetry.db
.tables
SELECT * FROM telemetry_events LIMIT 10;
```

---

## ðŸŽ“ Learning Path Summary

Here's the recommended order to understand everything:

### **Week 1: Basics**
1. âœ… Setup environment (Step 1)
2. âœ… Understand structure (Step 2)
3. âœ… Generate data (Step 3)
4. âœ… Basic analysis (Step 4)
5. âœ… Explore dashboard (Step 9)

### **Week 2: Intermediate**
1. âœ… SQL analysis (Step 5)
2. âœ… A/B testing (Step 6)
3. âœ… Code evaluation (Step 8)
4. âœ… Read METHODOLOGY.md

### **Week 3: Advanced**
1. âœ… Causal inference (Step 7)
2. âœ… Deep dive into code
3. âœ… Modify and experiment
4. âœ… Read SHOWCASE_SUMMARY.md

---

## ðŸ” Common Questions

### Q: What if I get an error?

**A:** Check:
1. Is your virtual environment activated? (Should see `(.venv)`)
2. Did you install requirements? (`pip install -r requirements.txt`)
3. Did you run `simulate` before other commands?
4. Check the error message - it usually tells you what's missing

### Q: Can I use real data instead of simulated?

**A:** Yes! Replace `simulate_telemetry.py` with code that loads your real data. The rest of the pipeline will work the same.

### Q: How do I add my own coding tasks?

**A:** 
1. Add a new task to `code-evaluation-pipeline/tasks/tasks.json`
2. Add a reference solution to `code-evaluation-pipeline/tasks/reference_solutions.py`
3. Add test function to `code-evaluation-pipeline/run_tests.py`
4. Add code generation logic to `code-evaluation-pipeline/generate_code.py`

### Q: What if I want to use a real AI model?

**A:** Edit `code-evaluation-pipeline/generate_code.py`:
1. Get an OpenAI API key
2. Uncomment the OpenAI API code
3. Replace the placeholder with real API calls

### Q: How do I customize the dashboard?

**A:** Edit `dashboard/app.py`:
- Add new pages in the `page` selector
- Add new visualizations using Plotly
- Modify filters and metrics

---

## ðŸŽ¯ Next Steps

Now that you've explored everything:

1. **Experiment**: Try changing parameters in the simulation
2. **Extend**: Add your own analyses or visualizations
3. **Learn**: Read METHODOLOGY.md for deeper understanding
4. **Apply**: Use these patterns in your own projects

---

## ðŸ“– Additional Resources

- **README.md**: Overview of the repository
- **METHODOLOGY.md**: Deep dive into methods and best practices
- **SHOWCASE_SUMMARY.md**: How this demonstrates role fit
- **code-evaluation-pipeline/evaluation_report.md**: Code evaluation methodology

---

## ðŸŽ‰ Congratulations!

You've now explored a complete Data Science workflow for AI developer tools:
- Data collection â†’ Analysis â†’ Statistical testing â†’ Visualization â†’ Recommendations

This is exactly what a Data Scientist at OpenAI Codex does every day!

Happy exploring! ðŸš€

